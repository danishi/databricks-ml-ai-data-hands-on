"""
Streamlit Databricks App: RAG ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ

ç¤¾å†…FAQãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ Vector Search ã§æ¤œç´¢ã—ã€LLM ãŒå›ç­”ã‚’ç”Ÿæˆã™ã‚‹ RAG ãƒãƒ£ãƒƒãƒˆã‚¢ãƒ—ãƒªã§ã™ã€‚
Databricks Apps ã¨ã—ã¦ãƒ‡ãƒ—ãƒ­ã‚¤ã—ã¦ä½¿ç”¨ã—ã¾ã™ã€‚

=== ã“ã®ã‚¢ãƒ—ãƒªã®ä»•çµ„ã¿ï¼ˆRAG ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼‰===

  ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè³ªå•ã‚’å…¥åŠ›
       â†“
  â‘  Databricks Vector Search ã§é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ¤œç´¢
       â†“
  â‘¡ æ¤œç´¢çµæœ + è³ªå• ã‚’ LLM ã«æ¸¡ã—ã¦å›ç­”ã‚’ç”Ÿæˆï¼ˆGenerationï¼‰
       â†“
  å›ç­” + å‚ç…§å…ƒãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¡¨ç¤º

=== å‰ææ¡ä»¶ ===
- Foundation Model APIs ãŒæœ‰åŠ¹ãªãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã§ Databricks Apps ã¨ã—ã¦ãƒ‡ãƒ—ãƒ­ã‚¤
- genai/03_vector_search_rag.py ã‚’å®Ÿè¡Œæ¸ˆã¿ï¼ˆVector Search Index ãŒä½œæˆæ¸ˆã¿ã§ã‚ã‚‹ã“ã¨ï¼‰

=== ãƒ‡ãƒ—ãƒ­ã‚¤æ–¹æ³• ===
  1. ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã€Œ+ æ–°è¦ã€â†’ã€Œã‚¢ãƒ—ãƒªã€ã‚’é¸æŠ
  2. ã€Œã‚«ã‚¹ã‚¿ãƒ ã‚¢ãƒ—ãƒªã‚’ä½œæˆã€ã‚’ã‚¯ãƒªãƒƒã‚¯
  3. ã‚¢ãƒ—ãƒªåã‚’å…¥åŠ›ï¼ˆä¾‹: rag-chat-appï¼‰
  4. ã€Œæ¬¡: è¨­å®šã€ã§ã€Œã‚µãƒ¼ãƒ“ãƒ³ã‚°ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã€ãƒªã‚½ãƒ¼ã‚¹ã‚’è¿½åŠ 
  5. ã€Œã‚¢ãƒ—ãƒªã®ä½œæˆã€ã‚’ã‚¯ãƒªãƒƒã‚¯
  6. ã‚¢ãƒ—ãƒªè©³ç´°ç”»é¢ã§ã€Œãƒ‡ãƒ—ãƒ­ã‚¤ã€â†’ app_rag/ ãƒ•ã‚©ãƒ«ãƒ€ã‚’é¸æŠ

=== ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ ===
  app_rag/
  â”œâ”€â”€ app.py              â† ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªï¼‰
  â”œâ”€â”€ app.yaml            â† ã‚¢ãƒ—ãƒªã®å®Ÿè¡Œè¨­å®š
  â””â”€â”€ requirements.txt    â† å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªä¸€è¦§
"""

import streamlit as st
from openai import OpenAI
from databricks.sdk import WorkspaceClient

# --- ãƒšãƒ¼ã‚¸è¨­å®š ---
st.set_page_config(
    page_title="ç¤¾å†…FAQ RAGãƒãƒ£ãƒƒãƒˆ",
    page_icon="ğŸ’¬",
    layout="centered",
)

# --- å®šæ•° ---
LLM_MODEL = "databricks-meta-llama-3-3-70b-instruct"
VS_INDEX_NAME = "main.default.rag_documents_index"


@st.cache_resource
def get_clients():
    """Databricks ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å–å¾—ï¼ˆDatabricks Apps ã§ã¯è‡ªå‹•èªè¨¼ï¼‰"""
    w = WorkspaceClient()
    openai_client = OpenAI(
        api_key=w.config.token,
        base_url=f"{w.config.host}/serving-endpoints",
    )
    return w, openai_client


def search_documents(query: str, top_k: int = 3) -> list[dict]:
    """Vector Search Index ã‚’ä½¿ã£ã¦ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ¤œç´¢"""
    w, _ = get_clients()
    results = w.vector_search_indexes.query_index(
        index_name=VS_INDEX_NAME,
        columns=["chunk_id", "title", "content"],
        query_text=query,
        num_results=top_k,
    )
    return [
        {
            "chunk_id": row[0],
            "title": row[1],
            "content": row[2],
            "score": row[3] if len(row) > 3 else 0,
        }
        for row in results.result.data_array
    ]


def generate_rag_response(query: str, search_results: list) -> str:
    """æ¤œç´¢çµæœã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦LLMã«å›ç­”ã‚’ç”Ÿæˆã•ã›ã‚‹"""
    _, openai_client = get_clients()

    context_parts = []
    for r in search_results:
        context_parts.append(f"ã€{r['title']}ã€‘\n{r['content']}")
    context = "\n\n".join(context_parts)

    system_prompt = (
        "ã‚ãªãŸã¯ç¤¾å†…FAQã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚\n"
        "ä»¥ä¸‹ã®ã€Œå‚è€ƒæƒ…å ±ã€ã®ã¿ã«åŸºã¥ã„ã¦è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚\n"
        "å‚è€ƒæƒ…å ±ã«è¨˜è¼‰ãŒãªã„å†…å®¹ã«ã¤ã„ã¦ã¯ã€Œã“ã®æƒ…å ±ã¯ç¤¾å†…FAQã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€ã¨å›ç­”ã—ã¦ãã ã•ã„ã€‚\n"
        "å›ç­”ã¯ç°¡æ½”ã‹ã¤æ­£ç¢ºã«ãŠé¡˜ã„ã—ã¾ã™ã€‚"
    )

    user_prompt = f"## å‚è€ƒæƒ…å ±\n\n{context}\n\n## è³ªå•\n\n{query}"

    response = openai_client.chat.completions.create(
        model=LLM_MODEL,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        max_tokens=512,
        temperature=0.0,
    )
    return response.choices[0].message.content


def main():
    st.title("ç¤¾å†…FAQ RAGãƒãƒ£ãƒƒãƒˆ")
    st.caption("Databricks Vector Search + LLM ã§ç¤¾å†…ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«åŸºã¥ã„ã¦å›ç­”ã—ã¾ã™ï¼ˆRAG: æ¤œç´¢æ‹¡å¼µç”Ÿæˆï¼‰")

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼: è¨­å®šæƒ…å ±ã‚’è¡¨ç¤º
    with st.sidebar:
        st.subheader("RAG è¨­å®š")
        st.text(f"Vector Search Index:\n{VS_INDEX_NAME}")
        st.text(f"LLM: {LLM_MODEL}")
        st.divider()
        st.caption("genai/03_vector_search_rag.py ã§ä½œæˆã—ãŸ Vector Search Index ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™")

    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®åˆæœŸåŒ–
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’è¡¨ç¤º
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])
            if "sources" in msg:
                with st.expander("å‚ç…§å…ƒãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ"):
                    for src in msg["sources"]:
                        st.write(f"- {src['title']} (ã‚¹ã‚³ã‚¢: {src['score']:.4f})")

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
    if prompt := st.chat_input("ç¤¾å†…åˆ¶åº¦ã«ã¤ã„ã¦è³ªå•ã—ã¦ãã ã•ã„ï¼ˆä¾‹: ãƒªãƒ¢ãƒ¼ãƒˆãƒ¯ãƒ¼ã‚¯ã¯ä½•æ—¥ã¾ã§ï¼Ÿï¼‰"):
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤ºãƒ»ä¿å­˜
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # RAGã§å›ç­”ã‚’ç”Ÿæˆ
        with st.chat_message("assistant"):
            with st.spinner("Vector Search ã§æ¤œç´¢ä¸­..."):
                search_results = search_documents(prompt, top_k=3)

            with st.spinner("å›ç­”ã‚’ç”Ÿæˆä¸­..."):
                answer = generate_rag_response(prompt, search_results)

            st.markdown(answer)

            # å‚ç…§å…ƒã‚’è¡¨ç¤º
            sources = [
                {"title": r["title"], "score": r["score"]}
                for r in search_results
            ]
            with st.expander("å‚ç…§å…ƒãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ"):
                for src in sources:
                    st.write(f"- {src['title']} (ã‚¹ã‚³ã‚¢: {src['score']:.4f})")

        # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä¿å­˜
        st.session_state.messages.append({
            "role": "assistant",
            "content": answer,
            "sources": sources,
        })


if __name__ == "__main__":
    main()
